{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Fashion-MNIST\n",
    "\n",
    "Now it's your turn to build and train a neural network. You'll be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a drop-in replacement for the MNIST dataset. MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. Fashion-MNIST is a set of 28x28 greyscale images of clothes. It's more complex than MNIST, so it's a better representation of the actual performance of your network, and a better representation of datasets you'll use in the real world.\n",
    "\n",
    "<img src='assets/fashion-mnist-sprite.png' width=500px>\n",
    "\n",
    "In this notebook, you'll build your own neural network. For the most part, you could just copy and paste the code from Part 3, but you wouldn't be learning. It's important for you to write the code yourself and get it to work. Feel free to consult the previous notebooks though as you work through this.\n",
    "\n",
    "First off, let's load the dataset through torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits or log-softmax from the forward pass. It's up to you how many layers you add and the size of those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your network architecture here\n",
    "\n",
    "class FashionMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "Now you should create your network and train it. First you'll want to define [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) ( something like `nn.CrossEntropyLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (typically `optim.SGD` or `optim.Adam`).\n",
    "\n",
    "Then write the training code. Remember the training pass is a fairly straightforward process:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get the training loss below 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the network, define the criterion and optimizer\n",
    "model = FashionMNIST()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-4a4b75f7f4b1>:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(self.fc2(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(2.3365, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.2132, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.9838, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.8470, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.6855, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.5411, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.3338, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.3068, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.3615, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.2991, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.2568, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.9890, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.0551, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.0371, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.0174, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.9713, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.9804, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.0624, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7289, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6839, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.9357, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6812, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.8836, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7406, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.0700, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.8490, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.0203, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7517, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7122, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.9871, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6314, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.8591, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6469, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.8276, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6529, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.8696, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.8324, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7535, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.8987, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7733, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7723, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6350, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5663, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.8016, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6855, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6028, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6057, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.9432, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6125, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7662, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6256, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5977, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5997, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7760, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6039, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6141, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5155, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.8211, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4390, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6847, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4591, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6652, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7088, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5582, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6744, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4596, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6196, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7804, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4790, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7170, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6594, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5794, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7031, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6393, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5981, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6819, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4560, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.8748, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6881, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5409, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4551, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5400, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5322, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5499, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4655, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5845, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6462, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5486, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5062, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3820, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5888, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4891, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5927, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7037, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5409, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4949, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7133, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6543, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5053, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4358, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5795, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7823, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5899, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5602, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4575, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3517, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5617, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7765, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5099, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6831, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4455, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5125, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4564, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6648, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4577, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5079, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5930, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3706, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3375, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6424, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5232, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5064, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4813, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5277, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6259, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5672, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6594, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3950, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6016, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7395, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4031, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7108, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7732, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4732, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5503, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5017, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4891, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3936, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5555, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.8113, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5093, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5021, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5232, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5584, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5693, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6124, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5990, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3857, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5609, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5374, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4591, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6949, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5027, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4340, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6159, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7278, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6637, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5777, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5747, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6501, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4507, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5692, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4988, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5417, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4808, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4562, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3640, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4157, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6687, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5804, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5830, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4784, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5312, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2885, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6800, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5732, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5336, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5598, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5158, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4201, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6156, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.4634, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5752, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6372, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4406, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3563, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4718, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5012, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4763, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4800, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6645, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5504, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5936, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3602, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5360, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5339, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3523, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6700, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2476, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4911, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3573, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5757, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6142, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3427, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4464, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3930, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4623, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4326, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3879, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5923, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6610, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7202, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3339, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5694, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4533, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5912, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5753, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3781, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6692, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5921, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3939, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4403, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5128, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5833, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5018, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4773, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4802, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5421, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7842, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6172, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6063, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6206, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3745, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4437, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6118, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4127, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5482, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5480, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6250, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4828, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4733, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6386, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5046, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5097, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5313, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6992, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5702, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4136, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4493, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4064, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5010, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4607, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4896, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4747, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4859, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5829, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4141, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4930, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5104, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7157, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4442, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5629, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4888, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7733, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6397, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4167, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3676, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5739, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3727, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4045, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6944, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4961, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6450, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6793, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4433, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5030, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4601, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4824, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5564, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2557, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3682, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3006, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5890, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3758, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5971, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5120, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5790, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7921, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4729, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2722, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5213, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6054, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3479, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4091, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3948, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4409, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4496, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3578, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5814, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5400, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5326, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6258, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5799, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3311, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3489, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5791, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5586, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5204, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3959, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5257, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3861, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4012, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4678, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3705, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3955, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3892, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6125, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5064, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6237, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3930, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4575, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4729, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6870, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3568, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4764, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6457, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4087, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3867, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5407, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5506, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7581, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4046, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4765, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4316, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4515, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5110, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3433, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5238, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4386, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4565, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3875, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4846, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5015, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4734, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3859, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6417, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3745, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5316, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4918, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5084, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6507, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4635, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4454, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5149, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5014, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5052, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.8071, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5421, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4436, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5811, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5669, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4322, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5700, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4013, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2583, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.3704, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5975, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4301, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3937, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4182, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4333, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5839, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3418, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4355, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4384, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4486, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6864, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2969, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6142, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3961, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5619, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5856, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7318, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4501, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5243, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4549, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5674, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3326, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4912, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4192, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6185, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3916, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3951, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4462, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5890, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2903, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5382, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6021, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4835, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4411, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3979, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4258, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5420, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3966, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4263, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3509, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3734, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5005, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4171, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3901, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6233, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5391, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4451, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5261, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3270, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5615, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3973, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3410, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4005, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4397, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3265, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3869, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4695, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3236, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5554, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4030, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4916, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5970, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4405, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5017, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4891, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5194, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6746, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2876, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4320, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4701, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5337, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5600, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4751, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4121, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6641, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3446, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4287, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4010, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4682, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3695, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4577, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4520, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6235, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3396, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4457, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4090, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4945, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6151, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5583, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4657, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6003, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5161, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3908, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3070, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5595, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4212, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3584, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3173, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4208, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3964, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3303, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5485, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6532, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5885, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4492, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2395, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4302, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3559, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5895, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4924, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4921, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6550, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4494, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3397, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4744, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2953, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4563, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4682, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3127, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3191, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4767, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4319, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5221, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4101, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6025, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3966, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3645, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5255, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4168, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4447, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7819, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3376, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5614, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5767, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5153, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2922, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4292, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3566, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5058, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4104, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3600, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4386, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4133, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2282, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3471, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3735, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2761, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5535, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6126, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3667, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3846, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6398, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3081, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3990, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6253, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4955, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5658, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5426, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4117, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4661, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4835, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3405, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3399, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4735, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2904, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4517, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4658, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3817, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3109, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3924, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4700, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3784, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2975, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6148, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3885, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5737, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5964, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2602, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4016, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3876, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3018, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6348, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4841, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5933, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5656, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.3728, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3064, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2384, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4149, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3365, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4841, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5786, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4266, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5155, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2952, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.1758, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4266, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4988, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4315, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3074, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4843, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4449, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5012, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3640, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4600, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3976, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3458, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3801, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4530, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4057, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5359, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3532, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4414, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5192, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4814, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3971, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5133, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4277, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3232, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3548, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3727, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6326, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5741, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5570, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2677, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5782, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5450, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3554, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5467, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5125, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3897, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3430, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5158, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6001, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3355, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3218, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3435, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2854, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2899, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4673, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5011, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2799, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4384, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5225, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4241, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5648, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3903, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4446, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4144, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4499, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3110, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3044, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4513, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6906, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3452, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4183, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5007, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4739, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4265, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3850, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5120, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5452, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4290, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6045, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5378, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5023, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3006, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5476, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2735, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3435, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4200, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4016, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6124, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4518, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3905, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4856, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4931, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4849, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4600, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2835, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2930, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4452, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3138, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4395, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3082, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6793, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5246, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4922, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3162, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5466, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5598, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5062, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3329, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2389, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4777, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4328, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3544, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7137, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3954, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4157, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3282, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3433, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3404, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4838, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2957, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3371, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.8161, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5375, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4933, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3958, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3330, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5074, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3952, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4235, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3800, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3616, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4753, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4641, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5730, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2559, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6462, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4277, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3387, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4895, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5688, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5476, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4444, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5334, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3750, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4241, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2809, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3863, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6850, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6473, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5286, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3609, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2311, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5929, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3865, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4977, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3901, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4101, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4916, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3111, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5778, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4908, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2665, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3431, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3927, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6671, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5101, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.4024, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6233, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4465, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2847, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4995, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2913, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2649, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3924, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3085, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4257, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3776, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3573, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5336, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4786, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5546, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3793, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3544, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3656, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6142, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3662, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3927, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3477, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6115, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4938, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4826, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6037, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4028, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4573, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4714, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3737, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4927, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5911, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6205, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4700, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4074, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5111, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7385, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4262, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6217, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4720, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4526, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4868, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4545, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6085, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5123, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3923, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2804, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2989, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3319, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5267, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4325, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3718, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3668, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4480, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5663, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4422, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3521, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4346, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4342, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4216, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5691, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3221, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5026, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3300, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3662, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4942, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5692, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5926, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6063, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3838, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5731, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2664, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3609, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4203, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4351, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4421, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4598, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5617, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3727, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5549, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5372, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7212, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3361, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2618, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4220, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3380, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2393, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4502, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5469, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3948, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4374, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3911, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3627, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5058, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5866, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3172, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4321, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5051, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6408, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4644, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4241, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4516, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3714, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4080, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4027, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4327, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5610, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4227, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3245, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4314, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3341, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3685, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2995, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3362, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4495, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4396, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2058, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4412, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4335, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4471, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4453, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6289, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2501, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3256, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3790, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3411, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6051, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4883, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4130, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3228, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4873, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4401, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4311, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4858, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3545, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3574, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3062, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3263, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3647, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4376, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.1684, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3815, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.7581, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6087, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4036, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4968, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4094, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3237, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5204, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3246, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5419, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4161, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4149, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4104, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2029, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3419, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3075, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5546, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5225, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3321, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3380, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3393, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4078, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3767, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4159, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4239, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3967, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.1663, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3273, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3264, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3682, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4973, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5244, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4072, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4972, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3508, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5295, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4462, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6248, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4291, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5239, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3696, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2346, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4761, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.3179, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3997, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3833, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4500, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2676, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4954, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5033, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2989, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4785, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5788, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4722, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5456, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5426, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3764, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2896, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4767, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4176, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3711, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4841, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4393, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2711, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2892, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4630, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4165, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.3961, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.6039, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4455, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2450, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.5206, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4812, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4381, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2935, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.4503, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(0.2209, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Train the network here\n",
    "\n",
    "for X_train, y_train  in trainloader:\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train.reshape(-1, 784))\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-4a4b75f7f4b1>:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(self.fc2(x))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 216,
       "width": 424
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "\n",
    "# Test out your network!\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "# TODO: Calculate the class probabilities (softmax) for img\n",
    "with torch.no_grad():\n",
    "    ps = torch.exp(model(img.view(-1, 784)))\n",
    "\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.resize_(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
